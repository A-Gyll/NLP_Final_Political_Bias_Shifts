{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
    "import gensim\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(base_model_path, trained_model_path_1, trained_model_path_2):\n",
    "    base_model = AutoModelForMaskedLM.from_pretrained(base_model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    trained_model_1 = AutoModelForMaskedLM.from_pretrained(trained_model_path_1, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    trained_model_2 = AutoModelForMaskedLM.from_pretrained(trained_model_path_2, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    \n",
    "    base_pipe = pipeline(\n",
    "        \"fill-mask\", \n",
    "        model=base_model, \n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(base_pipe)\n",
    "\n",
    "    trained_pipe_1 = pipeline(\n",
    "        \"fill-mask\", \n",
    "        model=trained_model_1, \n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    trained_pipe_2 = pipeline(\n",
    "        \"fill-mask\", \n",
    "        model=trained_model_2, \n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    return base_pipe, trained_pipe_1, trained_pipe_2\n",
    "\n",
    "def get_word_embedding(word, model):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        print(f\"'{word}' not found in vocabulary. Returning zero vector.\")\n",
    "        return np.zeros(300)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def choose_response(category_list, response_list, model):\n",
    "    results = []\n",
    "\n",
    "    all_categories = set(cat for categories in category_list for cat in categories)\n",
    "    category_embeddings = {category: get_word_embedding(category, model) for category in all_categories}\n",
    "\n",
    "    for categories, responses in zip(category_list, response_list):\n",
    "        predict_dict = {word: 0 for word in categories}\n",
    "\n",
    "        for response in responses:  \n",
    "            cur_embedding = get_word_embedding(response['token_str'].strip(), model)\n",
    "            cur_weight = response['score']\n",
    "\n",
    "            print(f\"Processing: '{response['token_str']}', Score: {cur_weight}\")\n",
    "            for category in categories: \n",
    "                cat_embd = category_embeddings[category]\n",
    "                sim = cosine_similarity(cat_embd, cur_embedding)\n",
    "                print(f\"Category: '{category}', Similarity: {sim:.4f}\")\n",
    "                predict_dict[category] += cur_weight * sim\n",
    "        \n",
    "        best_category = max(predict_dict, key=predict_dict.get)\n",
    "        results.append((best_category, predict_dict))  \n",
    "    return results\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    epsilon = 1e-10  # Small value to avoid log(0) or division by zero\n",
    "    p_values = np.array(list(p.values())) + epsilon\n",
    "    q_values = np.array(list(q.values())) + epsilon\n",
    "    return np.sum(p_values * np.log(p_values / q_values))\n",
    "\n",
    "def softmax(scores):\n",
    "    exp_scores = np.exp(list(scores.values())) \n",
    "    sum_exp_scores = np.sum(exp_scores)        \n",
    "    softmax_scores = {k: v / sum_exp_scores for k, v in zip(scores.keys(), exp_scores)}  # Normalize\n",
    "    return softmax_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.fill_mask.FillMaskPipeline object at 0x7fea222c11d0>\n",
      "Loading word2vec model\n",
      "\n",
      "Generating results\n",
      "\n",
      "Determining most likely response\n",
      "\n",
      "Processing: ' too', Score: 0.33642578125\n",
      "Category: 'very', Similarity: 0.6065\n",
      "Category: 'somewhat', Similarity: 0.3555\n",
      "Category: 'not', Similarity: 0.5153\n",
      "Processing: ' quite', Score: 0.27880859375\n",
      "Category: 'very', Similarity: 0.7689\n",
      "Category: 'somewhat', Similarity: 0.5678\n",
      "Category: 'not', Similarity: 0.4772\n",
      "Processing: ' more', Score: 0.1126708984375\n",
      "Category: 'very', Similarity: 0.3496\n",
      "Category: 'somewhat', Similarity: 0.3340\n",
      "Category: 'not', Similarity: 0.2271\n",
      "Processing: ' very', Score: 0.07989501953125\n",
      "Category: 'very', Similarity: 1.0000\n",
      "Category: 'somewhat', Similarity: 0.5031\n",
      "Category: 'not', Similarity: 0.3947\n",
      "Processing: ' fairly', Score: 0.068359375\n",
      "Category: 'very', Similarity: 0.6954\n",
      "Category: 'somewhat', Similarity: 0.5276\n",
      "Category: 'not', Similarity: 0.3593\n",
      "Processing: ' difficult', Score: 0.5771484375\n",
      "Category: 'difficult', Similarity: 1.0000\n",
      "Category: 'easy', Similarity: 0.5891\n",
      "Processing: ' hard', Score: 0.35009765625\n",
      "Category: 'difficult', Similarity: 0.6026\n",
      "Category: 'easy', Similarity: 0.4710\n",
      "Processing: ' impossible', Score: 0.019744873046875\n",
      "Category: 'difficult', Similarity: 0.7195\n",
      "Category: 'easy', Similarity: 0.5476\n",
      "Processing: ' challenging', Score: 0.0116119384765625\n",
      "Category: 'difficult', Similarity: 0.6389\n",
      "Category: 'easy', Similarity: 0.2773\n",
      "Processing: ' important', Score: 0.00849151611328125\n",
      "Category: 'difficult', Similarity: 0.5088\n",
      "Category: 'easy', Similarity: 0.4191\n",
      "Processing: ' very', Score: 0.8203125\n",
      "Category: 'very', Similarity: 1.0000\n",
      "Category: 'somewhat', Similarity: 0.5031\n",
      "Category: 'not', Similarity: 0.3947\n",
      "Processing: ' quite', Score: 0.0787353515625\n",
      "Category: 'very', Similarity: 0.7689\n",
      "Category: 'somewhat', Similarity: 0.5678\n",
      "Category: 'not', Similarity: 0.4772\n",
      "Processing: ' fairly', Score: 0.0308380126953125\n",
      "Category: 'very', Similarity: 0.6954\n",
      "Category: 'somewhat', Similarity: 0.5276\n",
      "Category: 'not', Similarity: 0.3593\n",
      "Processing: ' pretty', Score: 0.01702880859375\n",
      "Category: 'very', Similarity: 0.7626\n",
      "Category: 'somewhat', Similarity: 0.4721\n",
      "Category: 'not', Similarity: 0.4193\n",
      "Processing: ' rather', Score: 0.0159912109375\n",
      "Category: 'very', Similarity: 0.2664\n",
      "Category: 'somewhat', Similarity: 0.4043\n",
      "Category: 'not', Similarity: 0.3191\n",
      "Processing: ' extensively', Score: 0.407470703125\n",
      "Category: 'poorly', Similarity: 0.3698\n",
      "Category: 'well', Similarity: 0.3846\n",
      "Processing: ' closely', Score: 0.271484375\n",
      "Category: 'poorly', Similarity: 0.2398\n",
      "Category: 'well', Similarity: 0.3357\n",
      "Processing: ' intensely', Score: 0.04034423828125\n",
      "Category: 'poorly', Similarity: 0.2673\n",
      "Category: 'well', Similarity: 0.2184\n",
      "Processing: ' differently', Score: 0.02227783203125\n",
      "Category: 'poorly', Similarity: 0.4630\n",
      "Category: 'well', Similarity: 0.3705\n",
      "Processing: ' vigorously', Score: 0.0178985595703125\n",
      "Category: 'poorly', Similarity: 0.2455\n",
      "Category: 'well', Similarity: 0.2673\n",
      "Processing: ' quite', Score: 0.24609375\n",
      "Category: 'very', Similarity: 0.7689\n",
      "Category: 'somewhat', Similarity: 0.5678\n",
      "Category: 'not', Similarity: 0.4772\n",
      "Processing: ' too', Score: 0.238525390625\n",
      "Category: 'very', Similarity: 0.6065\n",
      "Category: 'somewhat', Similarity: 0.3555\n",
      "Category: 'not', Similarity: 0.5153\n",
      "Processing: ' so', Score: 0.19775390625\n",
      "Category: 'very', Similarity: 0.6327\n",
      "Category: 'somewhat', Similarity: 0.3561\n",
      "Category: 'not', Similarity: 0.5595\n",
      "Processing: ' very', Score: 0.1639404296875\n",
      "Category: 'very', Similarity: 1.0000\n",
      "Category: 'somewhat', Similarity: 0.5031\n",
      "Category: 'not', Similarity: 0.3947\n",
      "Processing: ' more', Score: 0.0401611328125\n",
      "Category: 'very', Similarity: 0.3496\n",
      "Category: 'somewhat', Similarity: 0.3340\n",
      "Category: 'not', Similarity: 0.2271\n",
      "Processing: ' difficult', Score: 0.50634765625\n",
      "Category: 'difficult', Similarity: 1.0000\n",
      "Category: 'easy', Similarity: 0.5891\n",
      "Processing: ' hard', Score: 0.44677734375\n",
      "Category: 'difficult', Similarity: 0.6026\n",
      "Category: 'easy', Similarity: 0.4710\n",
      "Processing: ' impossible', Score: 0.02520751953125\n",
      "Category: 'difficult', Similarity: 0.7195\n",
      "Category: 'easy', Similarity: 0.5476\n",
      "Processing: ' challenging', Score: 0.005451202392578125\n",
      "Category: 'difficult', Similarity: 0.6389\n",
      "Category: 'easy', Similarity: 0.2773\n",
      "Processing: ' tough', Score: 0.005123138427734375\n",
      "Category: 'difficult', Similarity: 0.6777\n",
      "Category: 'easy', Similarity: 0.4555\n",
      "Processing: ' very', Score: 0.85791015625\n",
      "Category: 'very', Similarity: 1.0000\n",
      "Category: 'somewhat', Similarity: 0.5031\n",
      "Category: 'not', Similarity: 0.3947\n",
      "Processing: ' quite', Score: 0.056610107421875\n",
      "Category: 'very', Similarity: 0.7689\n",
      "Category: 'somewhat', Similarity: 0.5678\n",
      "Category: 'not', Similarity: 0.4772\n",
      "Processing: ' more', Score: 0.0201873779296875\n",
      "Category: 'very', Similarity: 0.3496\n",
      "Category: 'somewhat', Similarity: 0.3340\n",
      "Category: 'not', Similarity: 0.2271\n",
      "Processing: ' pretty', Score: 0.01080322265625\n",
      "Category: 'very', Similarity: 0.7626\n",
      "Category: 'somewhat', Similarity: 0.4721\n",
      "Category: 'not', Similarity: 0.4193\n",
      "Processing: ' so', Score: 0.0069732666015625\n",
      "Category: 'very', Similarity: 0.6327\n",
      "Category: 'somewhat', Similarity: 0.3561\n",
      "Category: 'not', Similarity: 0.5595\n",
      "Processing: ' well', Score: 0.12286376953125\n",
      "Category: 'poorly', Similarity: 0.5581\n",
      "Category: 'well', Similarity: 1.0000\n",
      "Processing: ' already', Score: 0.072265625\n",
      "Category: 'poorly', Similarity: 0.3048\n",
      "Category: 'well', Similarity: 0.3065\n",
      "Processing: ' poorly', Score: 0.06787109375\n",
      "Category: 'poorly', Similarity: 1.0000\n",
      "Category: 'well', Similarity: 0.5581\n",
      "Processing: ' incorrectly', Score: 0.0528564453125\n",
      "Category: 'poorly', Similarity: 0.4455\n",
      "Category: 'well', Similarity: 0.2078\n",
      "Processing: ' correctly', Score: 0.03631591796875\n",
      "Category: 'poorly', Similarity: 0.5003\n",
      "Category: 'well', Similarity: 0.3811\n",
      "Processing: ' too', Score: 0.39306640625\n",
      "Category: 'very', Similarity: 0.6065\n",
      "Category: 'somewhat', Similarity: 0.3555\n",
      "Category: 'not', Similarity: 0.5153\n",
      "Processing: ' so', Score: 0.1689453125\n",
      "Category: 'very', Similarity: 0.6327\n",
      "Category: 'somewhat', Similarity: 0.3561\n",
      "Category: 'not', Similarity: 0.5595\n",
      "Processing: ' more', Score: 0.1236572265625\n",
      "Category: 'very', Similarity: 0.3496\n",
      "Category: 'somewhat', Similarity: 0.3340\n",
      "Category: 'not', Similarity: 0.2271\n",
      "Processing: ' very', Score: 0.11614990234375\n",
      "Category: 'very', Similarity: 1.0000\n",
      "Category: 'somewhat', Similarity: 0.5031\n",
      "Category: 'not', Similarity: 0.3947\n",
      "Processing: ' quite', Score: 0.0849609375\n",
      "Category: 'very', Similarity: 0.7689\n",
      "Category: 'somewhat', Similarity: 0.5678\n",
      "Category: 'not', Similarity: 0.4772\n",
      "Processing: ' difficult', Score: 0.591796875\n",
      "Category: 'difficult', Similarity: 1.0000\n",
      "Category: 'easy', Similarity: 0.5891\n",
      "Processing: ' hard', Score: 0.33740234375\n",
      "Category: 'difficult', Similarity: 0.6026\n",
      "Category: 'easy', Similarity: 0.4710\n",
      "Processing: ' impossible', Score: 0.0313720703125\n",
      "Category: 'difficult', Similarity: 0.7195\n",
      "Category: 'easy', Similarity: 0.5476\n",
      "Processing: ' easy', Score: 0.0111846923828125\n",
      "Category: 'difficult', Similarity: 0.5891\n",
      "Category: 'easy', Similarity: 1.0000\n",
      "Processing: ' challenging', Score: 0.00598907470703125\n",
      "Category: 'difficult', Similarity: 0.6389\n",
      "Category: 'easy', Similarity: 0.2773\n",
      "Processing: ' very', Score: 0.85986328125\n",
      "Category: 'very', Similarity: 1.0000\n",
      "Category: 'somewhat', Similarity: 0.5031\n",
      "Category: 'not', Similarity: 0.3947\n",
      "Processing: ' quite', Score: 0.032318115234375\n",
      "Category: 'very', Similarity: 0.7689\n",
      "Category: 'somewhat', Similarity: 0.5678\n",
      "Category: 'not', Similarity: 0.4772\n",
      "Processing: ' pretty', Score: 0.0276336669921875\n",
      "Category: 'very', Similarity: 0.7626\n",
      "Category: 'somewhat', Similarity: 0.4721\n",
      "Category: 'not', Similarity: 0.4193\n",
      "Processing: ' more', Score: 0.0202178955078125\n",
      "Category: 'very', Similarity: 0.3496\n",
      "Category: 'somewhat', Similarity: 0.3340\n",
      "Category: 'not', Similarity: 0.2271\n",
      "Processing: ' so', Score: 0.00925445556640625\n",
      "Category: 'very', Similarity: 0.6327\n",
      "Category: 'somewhat', Similarity: 0.3561\n",
      "Category: 'not', Similarity: 0.5595\n",
      "Processing: ' poorly', Score: 0.1285400390625\n",
      "Category: 'poorly', Similarity: 1.0000\n",
      "Category: 'well', Similarity: 0.5581\n",
      "Processing: ' incorrectly', Score: 0.0732421875\n",
      "Category: 'poorly', Similarity: 0.4455\n",
      "Category: 'well', Similarity: 0.2078\n",
      "Processing: ' wrong', Score: 0.0535888671875\n",
      "Category: 'poorly', Similarity: 0.2597\n",
      "Category: 'well', Similarity: 0.0907\n",
      "Processing: ' unfairly', Score: 0.041717529296875\n",
      "Category: 'poorly', Similarity: 0.3714\n",
      "Category: 'well', Similarity: 0.1856\n",
      "Processing: ' correctly', Score: 0.039215087890625\n",
      "Category: 'poorly', Similarity: 0.5003\n",
      "Category: 'well', Similarity: 0.3811\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function to run the pipeline and process results.\"\"\"\n",
    "# Configuration\n",
    "test_name = \"test_3\"\n",
    "base_model_path = '/home/ujx4ab/ondemand/NLP_Final_Political_Bias_Shifts/src/Local Models/FacebookAI_roberta-large'\n",
    "CNN_model_path = \"/home/ujx4ab/ondemand/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/FacebookAI_roberta-large/checkpoints/CNN_model_3_a_5_epoch/2024-12-15 22-17-39/checkpoint-6450\"\n",
    "Fox_model_path = \"/home/ujx4ab/ondemand/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/FacebookAI_roberta-large/checkpoints/Fox_model_3_a_5_epoch/2024-12-15 22-17-39/checkpoint-6600\"\n",
    "word_vec_path = '/home/ujx4ab/ondemand/NLP_Final_Political_Bias_Shifts/word2vec/GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "categories = {\n",
    "    \"test_1\": [\n",
    "        [\"good\", \"bad\"],\n",
    "        [\"very\", \"somewhat\", \"not\"],\n",
    "        [\"very\", \"somewhat\", \"not\"]\n",
    "    ],\n",
    "    \"test_3\": [\n",
    "        [\"very\", \"somewhat\", \"not\"],\n",
    "        [\"difficult\", \"easy\"],\n",
    "        [\"very\", \"somewhat\", \"not\"],\n",
    "        [\"poorly\", \"well\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "cloze_statements = {\n",
    "    \"test_1\": [\n",
    "        \"During the presidential campaign, artificial intelligence (AI) will be used for <mask>\",\n",
    "        \"I am <mask> concerned that artificial intelligence (AI) will be used to create and distribute fake information about the presidential candidates and campaigns\",\n",
    "        \"I am <mask> confident technology companies like Facebook, X (formerly Twitter), TikTok and Google, to prevent the misuse of their platforms to influence the election.\"\n",
    "    ],\n",
    "    \"test_3\": [\n",
    "        \"I have seen inaccurate news about the 2024 presidential election <mask> often.\",\n",
    "        \"I find it <mask> to determine what is true and what is not when getting news and information about the presidential campaign and candidates\",\n",
    "        \"I have been following news about candidates for the presidential election <mask> closely\",\n",
    "        \"The news media has covered the presidential election <mask>.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Loading model\\n\")\n",
    "base_pipe, CNN_pipeline, Fox_pipeline = load_models(base_model_path, CNN_model_path, Fox_model_path)\n",
    "\n",
    "print(\"Loading word2vec model\\n\")\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word_vec_path, binary=True)\n",
    "\n",
    "print(\"Generating results\\n\")    \n",
    "base_results = [base_pipe(statement) for statement in cloze_statements[test]]\n",
    "CNN_results = [CNN_pipeline(statement) for statement in cloze_statements[test]]\n",
    "Fox_results = [Fox_pipeline(statement) for statement in cloze_statements[test]]\n",
    "\n",
    "print(\"Determining most likely response\\n\")\n",
    "results_base_model = choose_response(categories[test], base_results, word2vec_model)\n",
    "CNN_fine_tuned_model = choose_response(categories[test], CNN_results, word2vec_model)\n",
    "Fox_fine_tuned_model = choose_response(categories[test], Fox_results, word2vec_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:  ('very', {'very': 0.5852647459541913, 'somewhat': 0.39180519840010675, 'not': 0.3880963543051621})\n",
      "CNN:  ('very', {'very': 0.637011854472803, 'somewhat': 0.39082881363719935, 'not': 0.4248174971071421})\n",
      "Fox:  ('very', {'very': 0.570014864904806, 'somewhat': 0.3478782877427875, 'not': 0.41155074682319537})\n",
      "base:  ('difficult', {'difficult': 0.8140547269522358, 'easy': 0.5224675089816628})\n",
      "CNN:  ('difficult', {'difficult': 0.8006565802304522, 'easy': 0.5263499530306035})\n",
      "Fox:  ('difficult', {'difficult': 0.8280949357731515, 'easy': 0.5375519666233686})\n",
      "base:  ('very', {'very': 0.9195476235609021, 'somewhat': 0.48814092481734406, 'not': 0.38466160898587987})\n",
      "CNN:  ('very', {'very': 0.9211490907073312, 'somewhat': 0.4780448556052761, 'not': 0.3786351143307911})\n",
      "Fox:  ('very', {'very': 0.9187117364954247, 'somewhat': 0.4740024585573792, 'not': 0.3761548927891454})\n",
      "base:  ('well', {'poorly': 0.2412988352557477, 'well': 0.2697112230325729})\n",
      "CNN:  ('well', {'poorly': 0.20019026919544558, 'well': 0.2077199389332236})\n",
      "Fox:  ('poorly', {'poorly': 0.21020420021341124, 'well': 0.11450902283149844})\n"
     ]
    }
   ],
   "source": [
    "for base, CNN, Fox in zip(results_base_model, CNN_fine_tuned_model, Fox_fine_tuned_model):\n",
    "    print(\"base: \", base)\n",
    "    print(\"CNN: \", CNN)\n",
    "    print(\"Fox: \", Fox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base softmax:  {'very': 0.3780496029282145, 'somewhat': 0.3115518771392319, 'not': 0.3103985199325536}\n",
      "CNN softmax:  {'very': 0.38601295954192666, 'somewhat': 0.30177686962904215, 'not': 0.3122101708290312}\n",
      "Fox softmax:  {'very': 0.3767529033729503, 'somewhat': 0.3017059799085498, 'not': 0.32154111671849994}\n",
      "KL Divergence scores: \n",
      "\tBase-CNN: 0.00024460966426517196\n",
      "\tBase-Fox: 0.0003565010210799472\n",
      "\tCNN-Fox: 0.00024960528361761547\n",
      "\n",
      "base softmax:  {'difficult': 0.5723846656894457, 'easy': 0.4276153343105543}\n",
      "CNN softmax:  {'difficult': 0.5681498686273262, 'easy': 0.4318501313726738}\n",
      "Fox softmax:  {'difficult': 0.5721290555698056, 'easy': 0.4278709444301943}\n",
      "KL Divergence scores: \n",
      "\tBase-CNN: 3.65750874815737e-05\n",
      "\tBase-Fox: 1.3345692628216393e-07\n",
      "\tCNN-Fox: 3.2315982619669434e-05\n",
      "\n",
      "base softmax:  {'very': 0.44736109574806765, 'somewhat': 0.2906033823386466, 'not': 0.26203552191328566}\n",
      "CNN softmax:  {'very': 0.4497767259491069, 'somewhat': 0.2887747752752847, 'not': 0.26144849877560844}\n",
      "Fox softmax:  {'very': 0.44999017045111844, 'somewhat': 0.2884484713468025, 'not': 0.26156135820207904}\n",
      "KL Divergence scores: \n",
      "\tBase-CNN: 1.2934473846142806e-05\n",
      "\tBase-Fox: 1.6154106529481466e-05\n",
      "\tCNN-Fox: 2.594759029532067e-07\n",
      "\n",
      "base softmax:  {'poorly': 0.4928973808566295, 'well': 0.5071026191433704}\n",
      "CNN softmax:  {'poorly': 0.49811759145928847, 'well': 0.5018824085407116}\n",
      "Fox softmax:  {'poorly': 0.5239055540698678, 'well': 0.4760944459301322}\n",
      "KL Divergence scores: \n",
      "\tBase-CNN: 5.4504388511228755e-05\n",
      "\tBase-Fox: 0.0019248435513322774\n",
      "\tCNN-Fox: 0.0013314837215368698\n",
      "\n",
      "Formatted Results: \n",
      "[\n",
      "    {\n",
      "        \"prediction\": \"very\",\n",
      "        \"softmax_base\": {\n",
      "            \"very\": 0.3780496029282145,\n",
      "            \"somewhat\": 0.3115518771392319,\n",
      "            \"not\": 0.3103985199325536\n",
      "        },\n",
      "        \"softmax_CNN\": {\n",
      "            \"very\": 0.38601295954192666,\n",
      "            \"somewhat\": 0.30177686962904215,\n",
      "            \"not\": 0.3122101708290312\n",
      "        },\n",
      "        \"softmax_Fox\": {\n",
      "            \"very\": 0.3767529033729503,\n",
      "            \"somewhat\": 0.3017059799085498,\n",
      "            \"not\": 0.32154111671849994\n",
      "        },\n",
      "        \"kl_divergence\": {\n",
      "            \"base_CNN\": 0.00024460966426517196,\n",
      "            \"base_Fox\": 0.0003565010210799472,\n",
      "            \"CNN_Fox\": 0.00024960528361761547\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"prediction\": \"difficult\",\n",
      "        \"softmax_base\": {\n",
      "            \"difficult\": 0.5723846656894457,\n",
      "            \"easy\": 0.4276153343105543\n",
      "        },\n",
      "        \"softmax_CNN\": {\n",
      "            \"difficult\": 0.5681498686273262,\n",
      "            \"easy\": 0.4318501313726738\n",
      "        },\n",
      "        \"softmax_Fox\": {\n",
      "            \"difficult\": 0.5721290555698056,\n",
      "            \"easy\": 0.4278709444301943\n",
      "        },\n",
      "        \"kl_divergence\": {\n",
      "            \"base_CNN\": 3.65750874815737e-05,\n",
      "            \"base_Fox\": 1.3345692628216393e-07,\n",
      "            \"CNN_Fox\": 3.2315982619669434e-05\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"prediction\": \"very\",\n",
      "        \"softmax_base\": {\n",
      "            \"very\": 0.44736109574806765,\n",
      "            \"somewhat\": 0.2906033823386466,\n",
      "            \"not\": 0.26203552191328566\n",
      "        },\n",
      "        \"softmax_CNN\": {\n",
      "            \"very\": 0.4497767259491069,\n",
      "            \"somewhat\": 0.2887747752752847,\n",
      "            \"not\": 0.26144849877560844\n",
      "        },\n",
      "        \"softmax_Fox\": {\n",
      "            \"very\": 0.44999017045111844,\n",
      "            \"somewhat\": 0.2884484713468025,\n",
      "            \"not\": 0.26156135820207904\n",
      "        },\n",
      "        \"kl_divergence\": {\n",
      "            \"base_CNN\": 1.2934473846142806e-05,\n",
      "            \"base_Fox\": 1.6154106529481466e-05,\n",
      "            \"CNN_Fox\": 2.594759029532067e-07\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"prediction\": \"well\",\n",
      "        \"softmax_base\": {\n",
      "            \"poorly\": 0.4928973808566295,\n",
      "            \"well\": 0.5071026191433704\n",
      "        },\n",
      "        \"softmax_CNN\": {\n",
      "            \"poorly\": 0.49811759145928847,\n",
      "            \"well\": 0.5018824085407116\n",
      "        },\n",
      "        \"softmax_Fox\": {\n",
      "            \"poorly\": 0.5239055540698678,\n",
      "            \"well\": 0.4760944459301322\n",
      "        },\n",
      "        \"kl_divergence\": {\n",
      "            \"base_CNN\": 5.4504388511228755e-05,\n",
      "            \"base_Fox\": 0.0019248435513322774,\n",
      "            \"CNN_Fox\": 0.0013314837215368698\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through the models\n",
    "for base, CNN, Fox in zip(results_base_model, CNN_fine_tuned_model, Fox_fine_tuned_model):\n",
    "    # Calculate softmax distributions\n",
    "    softmax_base = softmax(base[1])\n",
    "    softmax_CNN = softmax(CNN[1])\n",
    "    softmax_Fox = softmax(Fox[1])\n",
    "\n",
    "    # Calculate KL divergence scores\n",
    "    kl_div_base_CNN = kl_divergence(softmax_base, softmax_CNN)\n",
    "    kl_div_base_Fox = kl_divergence(softmax_base, softmax_Fox)\n",
    "    kl_div_CNN_Fox = kl_divergence(softmax_CNN, softmax_Fox)\n",
    "\n",
    "    # Print results\n",
    "    print(\"base softmax: \", softmax_base)\n",
    "    print(\"CNN softmax: \", softmax_CNN)\n",
    "    print(\"Fox softmax: \", softmax_Fox)\n",
    "    print(\"KL Divergence scores: \")\n",
    "    print(f\"\\tBase-CNN: {kl_div_base_CNN}\")\n",
    "    print(f\"\\tBase-Fox: {kl_div_base_Fox}\")\n",
    "    print(f\"\\tCNN-Fox: {kl_div_CNN_Fox}\")\n",
    "    print()\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    results.append({\n",
    "        \"prediction\": base[0],\n",
    "        \"softmax_base\": softmax_base,\n",
    "        \"softmax_CNN\": softmax_CNN,\n",
    "        \"softmax_Fox\": softmax_Fox,\n",
    "        \"kl_divergence\": {\n",
    "            \"base_CNN\": kl_div_base_CNN,\n",
    "            \"base_Fox\": kl_div_base_Fox,\n",
    "            \"CNN_Fox\": kl_div_CNN_Fox\n",
    "        }\n",
    "    })\n",
    "\n",
    "formatted_results = json.dumps(results, indent=4)\n",
    "print(\"Formatted Results: \")\n",
    "print(formatted_results)\n",
    "\n",
    "with open(f\"/home/ujx4ab/ondemand/NLP_Final_Political_Bias_Shifts/Results/{test}.json\", \"w\") as f:\n",
    "    f.write(formatted_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
