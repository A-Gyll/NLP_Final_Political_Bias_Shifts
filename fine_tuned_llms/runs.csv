experiment_id,model_name,from_pretrained_params,lora_config_params,quantization_params,tokenizer_params,training_args
2024-11-30 11-34-39,meta-llama_Llama-3.2-1B,"{'pretrained_model_name_or_path': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/src/Local Models/meta-llama_Llama-3.2-1B', 'torch_dtype': 'torch.float16'}","{'lora_alpha': 16, 'lora_dropout': 0.1, 'r': 64, 'bias': 'none', 'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>}",{},"{'truncation': True, 'padding': True, 'max_length': 384}","{'output_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 11-34-39', 'per_device_train_batch_size': 24, 'per_device_eval_batch_size': 16, 'max_steps': 10, 'evaluation_strategy': 'steps', 'save_strategy': 'steps', 'eval_steps': 10, 'save_steps': 10, 'load_best_model_at_end': True, 'metric_for_best_model': 'perplexity', 'greater_is_better': False, 'logging_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 11-34-39/metrics.json', 'logging_strategy': 'steps', 'fp16': True, 'learning_rate': 0.0001, 'lr_scheduler_type': 'constant', 'ddp_backend': 'nccl', 'optim': 'adamw_bnb_8bit'}"
2024-11-30 11-43-04,meta-llama_Llama-3.2-1B,"{'pretrained_model_name_or_path': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/src/Local Models/meta-llama_Llama-3.2-1B', 'torch_dtype': 'torch.float16'}","{'lora_alpha': 16, 'lora_dropout': 0.1, 'r': 64, 'bias': 'none', 'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>}",{},"{'truncation': True, 'padding': True, 'max_length': 384}","{'output_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 11-43-04', 'per_device_train_batch_size': 24, 'per_device_eval_batch_size': 16, 'max_steps': 10, 'evaluation_strategy': 'steps', 'save_strategy': 'steps', 'eval_steps': 10, 'save_steps': 10, 'load_best_model_at_end': True, 'metric_for_best_model': 'perplexity', 'greater_is_better': False, 'logging_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 11-43-04/metrics.json', 'logging_strategy': 'steps', 'fp16': True, 'learning_rate': 0.0001, 'lr_scheduler_type': 'constant', 'ddp_backend': 'nccl', 'optim': 'adamw_bnb_8bit', 'eval_on_start': True}"
2024-11-30 11-47-25,meta-llama_Llama-3.2-1B,"{'pretrained_model_name_or_path': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/src/Local Models/meta-llama_Llama-3.2-1B', 'torch_dtype': 'torch.float16'}","{'lora_alpha': 16, 'lora_dropout': 0.1, 'r': 64, 'bias': 'none', 'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>}",{},"{'truncation': True, 'padding': True, 'max_length': 384}","{'output_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 11-47-25', 'per_device_train_batch_size': 24, 'per_device_eval_batch_size': 16, 'num_train_epochs': 2, 'evaluation_strategy': 'steps', 'save_strategy': 'steps', 'eval_steps': 150, 'save_steps': 150, 'load_best_model_at_end': True, 'metric_for_best_model': 'perplexity', 'greater_is_better': False, 'logging_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 11-47-25/metrics.json', 'logging_strategy': 'steps', 'fp16': True, 'learning_rate': 0.0001, 'lr_scheduler_type': 'constant', 'ddp_backend': 'nccl', 'optim': 'adamw_bnb_8bit', 'eval_on_start': True}"
2024-11-30 17-12-35,meta-llama_Llama-3.2-1B,"{'pretrained_model_name_or_path': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/src/Local Models/meta-llama_Llama-3.2-1B', 'torch_dtype': 'torch.float16'}","{'lora_alpha': 16, 'lora_dropout': 0.1, 'r': 64, 'bias': 'none', 'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>}",{},"{'truncation': True, 'padding': True, 'max_length': 384}","{'output_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 17-12-35', 'per_device_train_batch_size': 24, 'per_device_eval_batch_size': 16, 'num_train_epochs': 2, 'evaluation_strategy': 'steps', 'save_strategy': 'steps', 'eval_steps': 150, 'save_steps': 150, 'load_best_model_at_end': True, 'metric_for_best_model': 'perplexity', 'greater_is_better': False, 'logging_dir': '/home/bhx5gh/Documents/NLP/NLP_Final_Political_Bias_Shifts/fine_tuned_llms/meta-llama_Llama-3.2-1B/checkpoints/2024-11-30 17-12-35/metrics.json', 'logging_strategy': 'steps', 'logging_steps': 150, 'fp16': True, 'ddp_backend': 'nccl', 'optim': 'adamw_bnb_8bit', 'eval_on_start': True}"
